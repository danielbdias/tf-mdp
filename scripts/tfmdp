#!/usr/bin/env python3

# This file is part of tf-mdp.

# tf-mdp is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# tf-mdp is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with tf-mdp. If not, see <http://www.gnu.org/licenses/>.


import rddlgym
import tfmdp

import argparse
import time


def parse_args():
    description = 'Probabilistic planning in continuous state-action MDPs using TensorFlow.'
    usage = '%(prog)s <planner> <rddl> <policy> <optimization> [-h] [--version]'

    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument(
        '-ld', '--logdir',
        type=str, default='/tmp/tfmdp',
        help='log directory for data summaries (default=/tmp/tfmdp)'
    )
    parser.add_argument(
        '--version',
        action='version',
        version=tfmdp.__version__
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='verbosity mode'
    )

    policy_group = parser.add_argument_group('Deep Reactive Policy')
    policy_group.add_argument(
        '-l', dest='layers',
        nargs='+',
        default=[],
        help='number of units in each hidden layer'
    )
    policy_group.add_argument(
        '-a', dest='activation',
        type=str, choices=['none', 'sigmoid', 'tanh', 'relu', 'relu6', 'crelu', 'elu', 'selu', 'softplus', 'softsign'],
        default='elu',
        help='activation function for hidden layers'
    )
    policy_group.add_argument(
        '-iln', '--input-layer-norm',
        action='store_true', default=False,
        help='input layer normalization flag'
    )

    optimization_group = parser.add_argument_group('Optimization')
    optimization_group.add_argument(
        '--optimizer',
        type=str, choices=['Adadelta', 'Adagrad', 'Adam', 'GradientDescent', 'ProximalGradientDescent', 'ProximalAdagrad', 'RMSProp'],
        default='RMSProp',
        help='loss optimizer (default=RMSProp)'
    )
    optimization_group.add_argument(
        '-lfn', '--loss-fn',
        type=str, choices=['linear', 'mse'],
        default='linear',
        help='loss function (default=linear)'
    )
    optimization_group.add_argument(
        '-lr', '--learning-rate',
        type=float, default=0.001,
        help='optimizer learning rate (default=0.001)'
    )

    command_parser = argparse.ArgumentParser(description=description, usage=usage, parents=[parser])

    subparsers = command_parser.add_subparsers(help='Planners')

    montecarlo_description = 'Monte Carlo trajectory sampling with pathwise derivatives'
    montecarlo_usage = 'tfmdp montecarlo <rddl> <policy> <optimization> <OPTIONS> [-v]'
    montecarlo_parser = subparsers.add_parser(
        'montecarlo',
        help=montecarlo_description + ' (--help for more options)',
        description='(tf-mdp) '+ montecarlo_description,
        usage=montecarlo_usage,
        parents=[parser]
    )
    montecarlo_parser.set_defaults(func=montecarlo)
    montecarlo_parser.add_argument(
        'rddl',
        type=str,
        help='RDDL file or rddlgym\'s domain/instance id'
    )
    montecarlo_hyperparameters_group = montecarlo_parser.add_argument_group('Hyper-parameters')
    montecarlo_hyperparameters_group.add_argument(
        '-b', '--batch-size',
        type=int, default=256,
        help='number of trajectories in a batch (default=256)'
    )
    montecarlo_hyperparameters_group.add_argument(
        '-hr', '--horizon',
        type=int, default=40,
        help='number of timesteps (default=40)'
    )
    montecarlo_hyperparameters_group.add_argument(
        '-e', '--epochs',
        type=int, default=200,
        help='number of training epochs (default=200)'
    )

    minimax_description = 'Robust trajectory sampling with stochastic reparameterization'
    minimax_usage = 'tfmdp minimax <rddl> <policy> <optimization> <OPTIONS> [-v]'
    minimax_parser = subparsers.add_parser(
        'minimax',
        help=minimax_description + ' (--help for more options)',
        description='(tf-mdp) '+ minimax_description,
        usage=minimax_usage,
        parents=[parser]
    )
    minimax_parser.set_defaults(func=minimax)
    minimax_parser.add_argument(
        'rddl',
        type=str,
        help='RDDL file or rddlgym\'s domain/instance id'
    )
    minimax_hyperparameters_group = minimax_parser.add_argument_group('Hyper-parameters')
    minimax_hyperparameters_group.add_argument(
        '-btrain', '--train-batch-size',
        type=int, default=256,
        help='number of training trajectories in a batch (default=256)'
    )
    minimax_hyperparameters_group.add_argument(
        '-btest', '--test-batch-size',
        type=int, default=1024,
        help='number of test trajectories in a batch (default=1024)'
    )
    minimax_hyperparameters_group.add_argument(
        '-hr', '--horizon',
        type=int, default=40,
        help='number of timesteps (default=40)'
    )
    minimax_hyperparameters_group.add_argument(
        '-ie', '--inner-epochs',
        type=int, default=30,
        help='number of training epochs for inner optimization level (default=30)'
    )
    minimax_hyperparameters_group.add_argument(
        '-oe', '--outter-epochs',
        type=int, default=200,
        help='number of training epochs for outter optimization level (default=200)'
    )
    minimax_hyperparameters_group.add_argument(
        '--noise', dest='regularization_rate',
        type=float, default=0.1,
        help='Noise log prob regularization rate (default=0.1)'
    )

    return command_parser.parse_args()


def print_parameters(args):
    if args.verbose:
        import tfmdp
        print()
        print('Running tf-mdp v{} ...'.format(tfmdp.__version__))
        print()
        print('>> RDDL:   {}'.format(args.rddl))
        print('>> logdir: {}'.format(args.logdir))
        print()
        print('>> Deep Reactive Policy:')
        print('layers = [{}]'.format(','.join(args.layers)))
        print('activation = {}'.format(args.activation))
        print('input  layer norm = {}'.format(args.input_layer_norm))
        print()
        print('>> Optimization:')
        print('optimizer     = {}'.format(args.optimizer))
        print('loss function = {}'.format(args.loss_fn))
        print('learning rate = {}'.format(args.learning_rate))
        print('batch size    = {}'.format(args.batch_size))
        print('horizon       = {}'.format(args.horizon))
        print('epochs        = {}'.format(args.epochs))
        print()


def load_model(args):
    compiler = rddlgym.make(args.rddl, mode=rddlgym.SCG)
    compiler.batch_mode_on()
    return compiler


def build_policy(compiler, args):
    from tfmdp.policy.feedforward import FeedforwardPolicy
    config = {
        'layers': args.layers,
        'activation': args.activation,
        'input_layer_norm': args.input_layer_norm
    }
    policy = FeedforwardPolicy(compiler, config)
    policy.build()
    return policy


def montecarlo(compiler, policy, args):
    from tfmdp.planning.pathwise import PathwiseOptimizationPlanner
    config = {
        'batch_size': args.batch_size,
        'horizon': args.horizon,
        'learning_rate': args.learning_rate,
        'logdir': args.logdir
    }
    planner = PathwiseOptimizationPlanner(compiler, config)
    planner.build(policy, loss=args.loss_fn, optimizer=args.optimizer)
    rewards = planner.run(args.epochs)
    return rewards


def minimax(compiler, policy, args):
    from tfmdp.planning.minimax import MinimaxOptimizationPlanner
    config = {
        'train_batch_size': args.train_batch_size,
        'test_batch_size': args.test_batch_size,
        'horizon': args.horizon,
        'learning_rate': args.learning_rate,
        'regularization_rate': args.regularization_rate,
        'logdir': args.logdir
    }
    planner = MinimaxOptimizationPlanner(compiler, config)
    planner.build(policy, loss=args.loss_fn, optimizer=args.optimizer)
    epochs = (args.outter_epochs, args.inner_epochs)
    _, rewards = planner.run(epochs)
    return rewards


def report_performance(rewards, horizon):
    reward = rewards[-1][1]
    print('>> Performance:')
    print('total reward = {:.4f}, reward per timestep = {:.4f}\n'.format(reward, reward / horizon))


if __name__ == '__main__':

    args = parse_args()
    print_parameters(args)

    print('>> Loading model ...')
    start = time.time()
    compiler = load_model(args)
    end = time.time()
    uptime = end - start
    print('Done in {:.6f} sec.'.format(uptime))
    print()

    print('>> Optimizing...')
    start = time.time()
    drp = build_policy(compiler, args)
    rewards = args.func(compiler, drp, args)
    end = time.time()
    uptime = end - start
    print()
    print('Done in {:.6f} sec.'.format(uptime))
    print()

    report_performance(rewards, args.horizon)

    print('tensorboard --logdir {}\n'.format(args.logdir))
